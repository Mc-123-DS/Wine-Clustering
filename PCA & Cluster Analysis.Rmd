---
title: "Wine Clustering Project"
author: "Micah Mayanja"
date: "2024-06-09"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Principal component analysis & Cluster analysis.

Unsupervised learning! Using Principal component analysis for dimension reduction and then clustering analysis.

The following descriptions are adapted from the UCI webpage:
These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.

```{r}
setwd("C:/Users/micah/OneDrive/Documents/R/Wine clustering")
data1 <- read.csv("~/R/Wine clustering/wine-clustering.csv")
```

```{r}
head(data1)
```

```{r}
dim(data1)
sum(is.na(data1))
```

The data contains 178 observations with 13 variables. It should also be noted that the data has no missing values.

### Principal Component analysis

```{r}
wine_data <- scale(data1)
pr.out <- prcomp(data1, scale=TRUE)
summary(pr.out)
```

A biplot showing how the variables are represented in a reduced dimensional space defined by the first two principal components. 

```{r}
library(ggbiplot)
ggbiplot(pr.out,obs.scale = 1, var.scale = 1,
         circle = TRUE,
         var.axes = TRUE) + 
  theme_minimal() +
  ggtitle('PCA Biplot of Wine Dataset (Standardized)')

#Alternatively
#options(repr.plot.width = 8, repr.plot.height = 6)  #Adjust plot size
#biplot(pr.out,scale = 0, col = c("blue", "red"),cex=0.8)
```

Determine the proportion of variance explained. 
```{r}
pr.var <- pr.out$sdev^2 #find variance from standard deviation
pve <- pr.var/sum(pr.var) #compute the proportion of variance explained

#scree plots
plot(pve, xlab="Principal components",ylab="Proportion of variance explained",
     ylim=c(0,1),main = "Proportion of variance explained with each component", cex.main = 0.9, type="b")
```

We choose the smallest number of principal components that are required in order to explain a sizable variation of the data (elbow in the scree plot). Therefore, we can reduce the dimensions from 13 variables to 3 principal components.

```{r}
#Extract the first 3 Principal components
Newdata <- as.data.frame(pr.out$x[,1:3])
head(Newdata)
dim(Newdata)
```

### Cluster Analysis

#### Hierarchical clustering 

```{r}
data.dist=dist(Newdata)

hcluster <- hclust(data.dist)

#Plot dendrogram 
plot(hcluster,main="Hierarchical Clustering Dendrogram", sub = "Complete Linkage",xlab ="",ylab ="", cex = 0.6)

#add horizontal line for clustering 
abline(h=8,col="red", lty = 2) 
```

#### K-means clustering

```{r}
set.seed(2)
km.out <- kmeans(Newdata,3,nstart =20)
#km.out
km.clusters <-  km.out$cluster

clusters <- as.factor(km.clusters)
# Create a PCA biplot with cluster colors
library(ggplot2)
ggplot(Newdata, aes(x = PC1, y = PC2, color = clusters)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("blue", "green", "red")) +  # Customize cluster colors
  labs(x = "PC1", y = "PC2", color = "Cluster") +
  ggtitle("Clustered Principal components")
```







